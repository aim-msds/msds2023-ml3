{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15011dbd-64fa-428b-87c5-78129d495d90",
   "metadata": {},
   "source": [
    "# Building a Simple ASR Application with Whisper ðŸ¤«ðŸ’¬\n",
    "---\n",
    "\n",
    "This notebook uses [Hugging Face](https://huggingface.co/docs/transformers/model_doc/whisper) and [Gradio](https://gradio.app/) to build a simple demo.\n",
    "\n",
    "Note that this application also requires the command-line tool [`ffmpeg`](https://ffmpeg.org/) to be installed on your system:\n",
    "\n",
    "```bash\n",
    "# on Ubuntu or Debian\n",
    "sudo apt update && sudo apt install ffmpeg\n",
    "\n",
    "# on Arch Linux\n",
    "sudo pacman -S ffmpeg\n",
    "\n",
    "# on MacOS using Homebrew (https://brew.sh/)\n",
    "brew install ffmpeg\n",
    "\n",
    "# on Windows using Chocolatey (https://chocolatey.org/)\n",
    "choco install ffmpeg\n",
    "\n",
    "# on Windows using Scoop (https://scoop.sh/)\n",
    "scoop install ffmpeg\n",
    "```\n",
    "\n",
    "You can also check out the [demo](https://huggingface.co/spaces/openai/whisper) hosted on the [Hugging Face Spaces](https://huggingface.co/spaces/launch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ada18f-2f0f-4371-ac6c-7a770426dfcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import gradio as gr\n",
    "\n",
    "pipe = pipeline(model=\"openai/whisper-small.en\")  \n",
    "\n",
    "def transcribe(audio):\n",
    "    text = pipe(audio)[\"text\"]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf929494-3855-4edd-b2e2-a5dbb2776420",
   "metadata": {},
   "outputs": [],
   "source": [
    "iface = gr.Interface(\n",
    "    fn=transcribe, \n",
    "    inputs=gr.Audio(source=\"microphone\", type=\"filepath\"), # swap source with \"upload\"\n",
    "    outputs=\"text\",\n",
    "    title=\"Whisper App\",\n",
    "    description=\"Realtime demo for automatic speech recognition using a Whisper model.\",\n",
    ")\n",
    "\n",
    "iface.launch(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851bf9cb-1ce5-405d-8b95-b113a5add815",
   "metadata": {},
   "source": [
    "## Model Information\n",
    "---\n",
    "\n",
    "You can also checkout Whisper's official [github repo](https://github.com/openai/whisper) for a more comprehensive (non-Hugging Face) tutorial.\n",
    "\n",
    "## Available models and languages\n",
    "\n",
    "There are five model sizes, four with English-only versions, offering speed and accuracy tradeoffs. Below are the names of the available models and their approximate memory requirements and relative speed. \n",
    "\n",
    "\n",
    "|  Size  | Parameters | English-only model | Multilingual model | Required VRAM | Relative speed |\n",
    "|:------:|:----------:|:------------------:|:------------------:|:-------------:|:--------------:|\n",
    "|  tiny  |    39 M    |     `tiny.en`      |       `tiny`       |     ~1 GB     |      ~32x      |\n",
    "|  base  |    74 M    |     `base.en`      |       `base`       |     ~1 GB     |      ~16x      |\n",
    "| small  |   244 M    |     `small.en`     |      `small`       |     ~2 GB     |      ~6x       |\n",
    "| medium |   769 M    |    `medium.en`     |      `medium`      |     ~5 GB     |      ~2x       |\n",
    "| large  |   1550 M   |        N/A         |      `large`       |    ~10 GB     |       1x       |\n",
    "\n",
    "The `.en` models for English-only applications tend to perform better, especially for the `tiny.en` and `base.en` models. We observed that the difference becomes less significant for the `small.en` and `medium.en` models.\n",
    "\n",
    "Whisper's performance varies widely depending on the language. The figure below shows a WER (Word Error Rate) breakdown by languages of the Fleurs dataset using the `large-v2` model (The smaller the numbers, the better the performance).\n",
    "\n",
    "![WER breakdown by language](https://raw.githubusercontent.com/openai/whisper/main/language-breakdown.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee4bf6a-fa0e-406f-9a28-84ede5cdc72b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:whisper]",
   "language": "python",
   "name": "conda-env-whisper-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
